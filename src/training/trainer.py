"""
Training module for English-Hausa translation model.
"""

import os
import logging
from typing import Dict, Any, Optional
import torch
from transformers import (
    AutoTokenizer, 
    AutoModelForSeq2SeqLM,
    DataCollatorForSeq2Seq,
    Seq2SeqTrainingArguments,
    Seq2SeqTrainer,
    EarlyStoppingCallback
)\nfrom datasets import DatasetDict\nimport numpy as np\nfrom ..utils import create_directories\n\n\nclass HausaTranslationTrainer:\n    \"\"\"Trainer for English-Hausa translation model.\"\"\"\n    \n    def __init__(self, config: Dict[str, Any]):\n        self.config = config\n        self.model_config = config.get('model', {})\n        self.training_config = config.get('training', {})\n        self.logger = logging.getLogger(__name__)\n        \n        # Initialize model and tokenizer\n        self.tokenizer = None\n        self.model = None\n        self.data_collator = None\n        \n        # Setup device\n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        self.logger.info(f\"Using device: {self.device}\")\n        \n    def initialize_model(self):\n        \"\"\"Initialize the model and tokenizer.\"\"\"\n        base_model = self.model_config.get('base_model', 'google/mt5-small')\n        \n        self.logger.info(f\"Loading model: {base_model}\")\n        \n        # Load tokenizer\n        self.tokenizer = AutoTokenizer.from_pretrained(base_model)\n        \n        # Load model\n        self.model = AutoModelForSeq2SeqLM.from_pretrained(base_model)\n        \n        # Setup data collator\n        self.data_collator = DataCollatorForSeq2Seq(\n            tokenizer=self.tokenizer,\n            model=self.model\n        )\n        \n        # Move model to device\n        self.model.to(self.device)\n        \n        self.logger.info(\"Model initialized successfully\")\n        \n    def preprocess_function(self, examples):\n        \"\"\"Preprocess examples for training.\"\"\"\n        source_lang = self.config.get('data', {}).get('source_lang', 'en')\n        target_lang = self.config.get('data', {}).get('target_lang', 'ha')\n        max_length = self.model_config.get('max_length', 512)\n        \n        inputs = [ex[source_lang] for ex in examples['translation']]\n        targets = [ex[target_lang] for ex in examples['translation']]\n        \n        # Add task prefix for mT5\n        if 'mt5' in self.model_config.get('base_model', '').lower():\n            inputs = [f\"translate English to Hausa: {text}\" for text in inputs]\n        \n        model_inputs = self.tokenizer(\n            inputs,\n            max_length=max_length,\n            truncation=True,\n            padding=True\n        )\n        \n        # Setup the tokenizer for targets\n        with self.tokenizer.as_target_tokenizer():\n            labels = self.tokenizer(\n                targets,\n                max_length=max_length,\n                truncation=True,\n                padding=True\n            )\n        \n        model_inputs[\"labels\"] = labels[\"input_ids\"]\n        return model_inputs\n    \n    def prepare_dataset(self, dataset: DatasetDict) -> DatasetDict:\n        \"\"\"Prepare dataset for training.\"\"\"\n        self.logger.info(\"Preprocessing dataset...\")\n        \n        tokenized_dataset = dataset.map(\n            self.preprocess_function,\n            batched=True,\n            remove_columns=dataset[\"train\"].column_names\n        )\n        \n        self.logger.info(\"Dataset preprocessing complete\")\n        return tokenized_dataset\n    \n    def compute_metrics(self, eval_pred):\n        \"\"\"Compute metrics during evaluation.\"\"\"\n        predictions, labels = eval_pred\n        \n        # Decode predictions and labels\n        decoded_preds = self.tokenizer.batch_decode(\n            predictions, skip_special_tokens=True\n        )\n        \n        # Replace -100 in the labels as we can't decode them\n        labels = np.where(labels != -100, labels, self.tokenizer.pad_token_id)\n        decoded_labels = self.tokenizer.batch_decode(\n            labels, skip_special_tokens=True\n        )\n        \n        # Simple length-based metric (can be improved with BLEU, etc.)\n        prediction_lens = [len(pred.split()) for pred in decoded_preds]\n        \n        return {\n            \"avg_prediction_length\": np.mean(prediction_lens)\n        }\n    \n    def train(self, dataset: DatasetDict, output_dir: str = None) -> None:\n        \"\"\"Train the translation model.\"\"\"\n        if not self.model or not self.tokenizer:\n            raise ValueError(\"Model not initialized. Call initialize_model() first.\")\n        \n        # Set output directory\n        if output_dir is None:\n            output_dir = os.path.join(\n                self.config.get('paths', {}).get('models_dir', './models'),\n                self.model_config.get('name', 'english-hausa-translator')\n            )\n        \n        # Create directories\n        os.makedirs(output_dir, exist_ok=True)\n        \n        # Prepare dataset\n        tokenized_dataset = self.prepare_dataset(dataset)\n        \n        # Setup training arguments\n        training_args = Seq2SeqTrainingArguments(\n            output_dir=output_dir,\n            num_train_epochs=self.training_config.get('num_epochs', 10),\n            per_device_train_batch_size=self.training_config.get('batch_size', 16),\n            per_device_eval_batch_size=self.training_config.get('batch_size', 16),\n            warmup_steps=self.training_config.get('warmup_steps', 1000),\n            weight_decay=0.01,\n            logging_dir=f\"{output_dir}/logs\",\n            logging_steps=self.training_config.get('logging_steps', 100),\n            evaluation_strategy=self.training_config.get('evaluation_strategy', 'steps'),\n            eval_steps=self.training_config.get('eval_steps', 500),\n            save_strategy=self.training_config.get('save_strategy', 'epoch'),\n            load_best_model_at_end=True,\n            metric_for_best_model=\"eval_loss\",\n            predict_with_generate=True,\n            fp16=torch.cuda.is_available(),  # Enable mixed precision if GPU available\n            dataloader_pin_memory=False,\n            gradient_checkpointing=True,\n            learning_rate=self.training_config.get('learning_rate', 5e-5),\n        )\n        \n        # Setup trainer\n        trainer = Seq2SeqTrainer(\n            model=self.model,\n            args=training_args,\n            train_dataset=tokenized_dataset[\"train\"],\n            eval_dataset=tokenized_dataset.get(\"validation\"),\n            tokenizer=self.tokenizer,\n            data_collator=self.data_collator,\n            compute_metrics=self.compute_metrics,\n            callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n        )\n        \n        # Start training\n        self.logger.info(\"Starting training...\")\n        trainer.train()\n        \n        # Save the final model\n        trainer.save_model()\n        self.tokenizer.save_pretrained(output_dir)\n        \n        self.logger.info(f\"Training completed. Model saved to {output_dir}\")\n        \n        # Evaluate on test set if available\n        if \"test\" in tokenized_dataset:\n            self.logger.info(\"Evaluating on test set...\")\n            test_results = trainer.evaluate(eval_dataset=tokenized_dataset[\"test\"])\n            self.logger.info(f\"Test results: {test_results}\")\n        \n        return trainer